\documentclass[a4paper, sans, 11pt]{moderncv}

\moderncvstyle{banking}
\moderncvcolor{black}
 
\usepackage{multicol}
\usepackage{lipsum}
\usepackage[hyperref]{}
\usepackage[scale=0.8, top=.5cm, bottom=.5cm, left=.8cm, right=.8cm]{geometry}

\usepackage{fontawesome5}
\usepackage{lmodern}

\usepackage{etoolbox}
\patchcmd{\makehead}% <cmd>
  {1\textwidth}% <search>
  {\textwidth}% <replace>
  {}{}
  
\name{Aleksei G Sorokin}{}                           
%\address{Chicago, IL, USA}{}{}
\phone[mobile]{+1~(630)~297~6261}                   
\email{asorokin@hawk.iit.edu}
\homepage{alegresor.github.io}
\social[linkedin]{aleksei-sorokin}
\social[github]{alegresor}
\social[googlescholar]{akk3XSEAAAAJ}
\extrainfo{US Citizen}

\usepackage[
    backend=biber,
    %defernumbers=true,
    sorting=ydnt,
    maxbibnames=99
  ]{biblatex}
\addbibresource{../../ags/meta/ags.bib}

\newcommand*{\newentry}[3][.25em]{%
  \begin{tabular}[t]{@{}p{0.15\textwidth}@{\hspace{\separatorcolumnwidth}}}%
    \raggedleft\hintstyle{#2}%
  \end{tabular}%
  \begin{tabular}[t]{@{}p{\dimexpr0.85\textwidth-\separatorcolumnwidth}@{}}
     #3
  \end{tabular}%
  \par\addvspace{#1}}

\begin{document}

\makecvtitle
\vspace{-1cm}

%\subsection{Background}
\newentry{Research}{Scientific Machine Learning, Gaussian Processes, Quasi-Monte Carlo, Probabilistic Numerics}
\newentry{Programming}{Python (PyTorch, GPyTorch, Pandas, Matplotlib), Julia, C, MATLAB, R, SQL, Wolfram} 
\newentry{Tools}{AWS (SageMaker, EC2), GitHub (general, actions, pages), \LaTeX, Docker}

\subsection{Education}
\newentry{\normalfont{2021 - 2026}}{\textbf{PhD in Applied Math.} Illinois Institute of Technology (IIT). GPA $3.89 / 4$. Advisor \emph{Fred J Hickernell}.}
\newentry{\normalfont{2017 - 2021}}{\textbf{Master of Data Science.} IIT. Summa Cum Laude. GPA $3.94 / 4$.}
\newentry{\normalfont{2017 - 2021}}{\textbf{B.S. in Applied Math, Minor in Computer Science.} IIT. Summa Cum Laude. GPA $3.94 / 4$.}

\subsection{Experiences}
\newentry{\normalfont{Jan - Dec 2025}}{\textbf{DOE SCGSR Fellow in Applied Mathematics} at \textbf{Sandia National Laboratory} in Livermore, CA. I am researching Gaussian process based scientific ML models for machine precision PDE solutions. I am also developing fast, scalable multi-task Gaussian processes for multi-fidelity modeling. We are preparing publications and open-source software with HPC support such as \texttt{FastGPs} below.}
\newentry{\normalfont{Summer 2024}}{\textbf{Scientific Machine Learning Researcher} at \textbf{FM (Factory Mutual Insurance Company).} I built scientific ML models, including Physics Informed Neural Networks (PINNs) and Deep Operator Networks (DeepONets), for solving Radiative Transport Equations (RTEs) used to speed up CFD fire dynamics simulations. Resulted in publication of \citetitle{sorokin.RTE_DeepONet}.}
\newentry{\normalfont{Summer 2023}}{\textbf{Graduate Intern} at \textbf{Los Alamos National Laboratory.} I modeled the solution processes of PDEs with random coefficients using efficient and error aware Gaussian processes. Resulted in publication of \citetitle{sorokin.gp4darcy}.}
\newentry{\normalfont{Summer 2022}}{\textbf{Givens Associate Intern} at \textbf{Argonne National Laboratory}. I researched methods to efficiently estimate failure probability using Monte Carlo with non-parametric importance sampling. Resulted in publication of \citetitle{sorokin.adaptive_prob_failure_GP}.}
\newentry{\normalfont{Summer 2021}}{\textbf{ML Engineer Intern} at \textbf{SigOpt, an Intel Company}. I developed novel meta-learning techniques for model-aware hyperparameter tuning via Bayesian optimization. In a six person ML engineering team, I contributed production code and learned key elements of the AWS stack. Resulted in publication of \citetitle{sorokin.sigopt_mulch}.}
\newentry{\normalfont{2021 - 2024}}{\textbf{Teaching Assistant} at \textbf{IIT}. I led reviews for PhD qualifying exams in analysis and computational math.}
% \newentry{2018 - 2021}{\textbf{Lead Developer} of \textbf{DNNB: The Deep Neural Network Builder in Python.} This research package implements deep learning models from scratch in Python. See \itlink{github.com/alegresor/DNNB}{https://github.com/alegresor/DNNB}.}
% \newentry{2018 - Present}{\textbf{Administrative Assistant} for \textbf{The Center for Interdisciplinary Scientific Computation at IIT}. I scheduled lecture series and maintained information on the CISC website at \itlink{cos.iit.edu/cisc/}{https://cos.iit.edu/cisc/}.}
% \newentry{2018 - 2019}{\textbf{Instructor} for the \textbf{STARS Computing Corp's Computer Discover Program.} I developed a curriculum for middle school and high school girls to learn programmatic thinking with Python.}

\subsection{Open-Source Software}
\newentry{\texttt{QMCPy}}{\textbf{Quasi-Monte Carlo Python Software} (\href{https://qmcsoftware.github.io/QMCSoftware}{qmcsoftware.github.io/QMCSoftware}), lead developer. This package provides high quality quasi-random sequence generators, automatic variable transformations, adaptive stopping criteria algorithms, and diverse use cases. Over the past five years, this project has grown to dozens of collaborators and multiple publications \cite{sorokin.2025.ld_randomizations_ho_nets_fast_kernel_mats,choi.challenges_great_qmc_software,choi.QMC_software,sorokin.MC_vector_functions_integrals,sorokin.QMC_IS_QMCPy,hickernell.qmc_what_why_how,jain.bernstein_betting_confidence_intervals}.}
\newentry{\texttt{FastGPs}}{\textbf{Scalable Gaussian Process Regression in Python} (\href{https://alegresor.github.io/fastgps}{alegresor.github.io/fastgps}). Gaussian process regression (GPR) models typically require $\mathcal{O}(n^2)$ storage and $\mathcal{O}(n^3)$ computations. \texttt{FastGPs} implements GPR which requires only $\mathcal{O}(n)$ storage and $\mathcal{O}(n \log n)$ computations by pairing certain quasi-random sampling locations with matching kernels to yield structured Gram matrices. We support GPU scaling, batched inference, robust hyperparameter optimization, and multi-task GPR.}
\newentry{\texttt{QMCGenerators}}{\textbf{Quasi-Random Sequence Generators in Julia} (\href{https://alegresor.github.io/QMCGenerators.jl}{{alegresor.github.io/QMCGenerators.jl}}). This package includes routines to generate and randomize quasi-random sequences used in Quasi-Monte Carlo. Supported low discrepancy sequences include lattices with random shifts and digital nets (e.g. Sobol' points) with random digital shifts, linear matrix scrambling, nested uniform scrambling, and higher order construction through digital interlacing. These features are also supported in \texttt{QMCPy}.}
\newentry{\texttt{AI on HPC}}{\textbf{AI Driven Science on Supercomputers Course} at \textbf{Argonne National Laboratory}. Key topics included handling large scale data pipelines and parallel training for neural networks. %Coursework at \itlink{github.com/alegresor/ai-science-training-series}{https://github.com/alegresor/ai-science-training-series}.
}

\subsection{Awards}
%\newentry{\normalfont{2025}}{\textbf{DOE SCGSR Fellow in Applied Mathematics}, Sandia National Laboratory at Livermore.}
\newentry{\normalfont{2025}}{\textbf{Karl Menger Student Award for Exceptional Scholarship (Graduate)}, IIT.}
\newentry{\normalfont{2024}}{\textbf{College of Computing Excellence in Dissertation Research}, IIT.}
\newentry{\normalfont{2024}}{\textbf{Teaching Assistant Award}, IIT.}
\newentry{\normalfont{2023}}{\textbf{Outstanding Math Poster}, Los Alamos National Laboratory.}
%\newentry{\normalfont{2021}}{\textbf{Best Manuscript}, IIT Undergraduate Research Journal.}
%\newentry{\normalfont{2020}}{\textbf{Karl Menger Student Award for Exceptional Scholarship}, IIT.}
%\newentry{\normalfont{2017 - Present}}{\textbf{Deans List Member}, IIT.}

% \subsection{Coursework}
% \newentry{Math}{
%     Applied Analysis I/II,
%     Computational Math,
%     Probability, 
%     Statistics, 
%     Applied Statistics,
%     Bayesian Computational Statistics, 
%     Statistical Learning, 
%     Monte Carlo Methods in Finance,
%     Mathematical Methods for Algorithmic Trading,
%     Numerical Methods for PDEs,
%     Reliable Mathematical Software, 
%     Linear Optimization, 
%     Computational Algebraic Geometry} 
% \newentry{Computer Science}{
%     Big Data Technologies,
%     Data Preparation and Analysis,
%     Database Organization,
%     Big Data Visualization,
%     Systems Programming, 
%     Computer Organization and Assembly,
%     Data Structures and Algorithms, 
%     Object Oriented Programming I/II.}


% \subsection{References}
% \newentry{\href{mailto:hickernell@iit.edu}{hickernell@iit.edu}}{\textbf{Fred J. Hickernell, PhD} Vice Provost for Research and Professor of Applied Math, IIT.}
% \newentry{\href{mailto:nickh@lanl.gov}{nickh@lanl.gov}}{\textbf{Nicolas W. Hengartner, PhD} Senior Scientist, Los Alamos National Laboratory.}
% \newentry{\href{mailto:mikemccourt1234@gmail.com}{mikemccourt1234@gmail.com}}{\textbf{Michael J. McCourt, PhD} Co-Founder and CTO at Distributional.}
% \newentry{\href{mailto:vhebbur@anl.gov}{vhebbur@anl.gov}}{\textbf{Vishwas Rao, PhD} Assistant Computational Mathematician, Argonne National Laboratory.}

%%%%% COVER LETTER
%\clearpage
%\recipient{HR Department}{Corporation\\123 Pleasant Lane\\12345 City, State} % Letter recipient
%\date{\today} % Letter date
%\opening{Dear Sir or Madam,} % Opening greeting
%\closing{Sincerely yours,} % Closing phrase
%\enclosure[Attached]{curriculum vit\ae{}} % List of enclosed documents
%\makelettertitle % Print letter title
%\lipsum[1-3] % Dummy text
%\makeletterclosing % Print letter signature

\printbibliography[title={Publications}]

\end{document}
